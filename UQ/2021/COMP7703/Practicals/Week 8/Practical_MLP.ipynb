{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4702/7703 Prac 7: MNIST Deep Learning with TensorFlow\n",
    "This notebook will allow you to create deep networks and MLPs for the MNIST dataset using tensorflow. As the course assumes no python knowledge I have written some code to do the implementation for you.\n",
    "If you are not using a lab computer to do this practical, you will need to install TensorFlow on your machine before continuing. See [here](https://www.tensorflow.org/install/) for more information on how to do this.\n",
    "\n",
    "# Disclaimer - this code has been tested on Ubuntu16.04 and Windows10 only.\n",
    "## Lets get cracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prac7ConvMLPModel import *\n",
    "from SupportCode.Helpers import *\n",
    "import numpy as np\n",
    "# Set seed so randomly generated values for hyper-parameters are predictable\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "[x_train, y_train], [x_test, y_test] = mnist\n",
    "\n",
    "# Flatten input arrays from 28x28 to 784 for x_train and x_test\n",
    "x_train = x_train.reshape(len(x_train), 784)\n",
    "x_test = x_test.reshape(len(x_test), 784)\n",
    "\n",
    "# Concatenate x_train and y_train in order to randomly shuffle whole dataset (VERY IMPORTANT - used for K-Fold CV)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "train = np.concatenate((x_train, y_train), axis=1)\n",
    "np.random.shuffle(train)\n",
    "# Resplit x_train and y_train\n",
    "x_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "# One hot encoding for y_train to be able to train neural net\n",
    "shape = (y_train.size, y_train.max() + 1)\n",
    "one_hot = np.zeros(shape)\n",
    "rows = np.arange(y_train.size)\n",
    "one_hot[rows, y_train] = 1\n",
    "y_train = one_hot\n",
    "\n",
    "# One hot encoding for y_train to be able to train neural net\n",
    "shape = (y_test.size, y_test.max() + 1)\n",
    "one_hot = np.zeros(shape)\n",
    "rows = np.arange(y_test.size)\n",
    "one_hot[rows, y_test] = 1\n",
    "y_test = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will create an MLP with default settings. Running this the first time will be a bit slow as it will download the MNIST dataset. The number of training steps has been set to a small number in order to verify the code is working and for you to see what the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify modified code works fine!\n",
    "# data = [x_train, y_train, x_test, y_test]\n",
    "# prac7ConvMLPModel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that two new TensorBoard tabs have opened in your browser. These display a variety of information about the network parameters. One tab indicates the test set summaries and accuracies and the other tab is the train set summaries. In the bottom left corner of the page you will see either \"MODELtrainX\" or \"MODELtestX\". X indicates the run number - this simply corresponds to the order that you run them in. MODEL is just the model name - either \"MLP\" or \"convNet\".\n",
    "\n",
    "To open a previous TensorBoard simply call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openTensorBoardAtIndex(\"MLP\", \"GradientDescent\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you go through the prac make sure to keep track of your settings for a given run - if you lose track you'll have to delete the folder and start again. -Each file is in the order of 10s-100s of MBs so you might need to delete some.\n",
    "\n",
    "A particularly interesting part of the tensorboard is the images tab. This shows the weight values as a greyscale image for each layer. You can move the sliding bar to see how the weights change over each iteration.\n",
    "\n",
    "# Configure your MLP\n",
    "By default prac7ConvMLPModel() will generate an MLP with one hidden layer that consists of 500 neurons. It will use stochastic gradient decent to optimise. You'll also notice that it doesn't perform very well.\n",
    "\n",
    "To change this you will need to change the optimiser values and the hidden layer values. Your prac 6 MLP might be a good start... unless it was bad ><.\n",
    "\n",
    "## Lets take a look at the optimisers\n",
    "In this prac you can use the following  optimisers: GradientDescent, Adam, RMSProp, Momentum, and Adagrad.\n",
    "\n",
    "Adam seems to be the most popular at the moment, followed by RMSProp.\n",
    "\n",
    "The following code should help you in configuring your optimisers. If you don't know what a parameter for a particular optimiser does remember that google is your friend. If you want to take a look at the tensorflow documentation check out the [TensorFlow documention](https://www.tensorflow.org/api_guides/python/train) \n",
    "\n",
    "**Don't** forget to *change* these values (srsly they will throw errors ;))!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP is pretty easy to set up as you only need to chose the layer layout. You could also change the activation function but by default it's the rectilinear unit or RELU. Check out the [TensorFlow documention](https://www.tensorflow.org/api_guides/python/)  for which functions you can use and then change the act parameter in the function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "### (a)\n",
    "Compare the performance of the different optimisers using the MLP topology you chose in Prac 6 Q3. \n",
    "\n",
    "### Instructions\n",
    "* Use a **table** to present your results including the hyper-parameters selected. \n",
    "* Describe in **at most** 250 words your methodology for selecting hyper-parameters.\n",
    "* Discuss in **at most** 150 words what attributes of each optimiser might make it perform better or worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up k-Fold Cross-Validation function thats splits training set into training and validation sets (leaving test set untouched for final models only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to split training set into 1/5 = 20% validation sets and 4/5 = 80% training sets for hyper-parameter\n",
    "# optimisation\n",
    "k = 5\n",
    "\n",
    "def k_fold_cv(k, i, x_train, y_train):\n",
    "    start_idx = int((i - 1)/k*len(x_train))\n",
    "    end_idx = int(i/k*len(x_train))\n",
    "    valid_x = x_train[start_idx:end_idx, :]\n",
    "    valid_y = y_train[start_idx:end_idx, :]\n",
    "    \n",
    "    train_x = np.delete(x_train, [range(start_idx, end_idx)], axis=0)\n",
    "    train_y = np.delete(y_train, [range(start_idx, end_idx)], axis=0)\n",
    "    return train_x, train_y, valid_x, valid_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activationFunction = tf.nn.relu\n",
    "MLPTopology={}\n",
    "# Use 2 hidden layers with 500 neurons in each layer\n",
    "MLPTopology['hiddenDims'] = [500, 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Search k-Fold CV for Hyper-Parameter Optimisation\n",
    "\n",
    "[Reference](https://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) - paper by Dr. Bengio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation dictionary for Gradient Descent\n",
    "optDicGD = {}\n",
    "optDicGD[\"optMethod\"] = \"GradientDescent\"\n",
    "\n",
    "learning_rates = np.random.uniform(low=0.0001, high=0.001, size=5)\n",
    "\n",
    "print(f\"Learning rates = {learning_rates}\")\n",
    "\n",
    "for i in range(1, k + 1):\n",
    "    data = k_fold_cv(k, i, x_train, y_train)\n",
    "    optDicGD[\"learning_rate\"] = learning_rates[i-1]\n",
    "    \n",
    "    \n",
    "    # Evaluate Gradient Descent optimiser with varying learning rates\n",
    "    prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicGD, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Gradient Descent model on test set (unseen data)\n",
    "data = [x_train, y_train, x_test, y_test]\n",
    "optDicGD[\"learning_rate\"] = learning_rates[2]\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicGD, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openTensorBoardAtIndex(\"MLP\", \"GradientDescent\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation dictionary for Momentum\n",
    "optDicM = {}\n",
    "optDicM[\"optMethod\"] = \"Momentum\"\n",
    "\n",
    "# Have already chosen 5 random learning rates, just choose 5 random momentums\n",
    "momentums = np.random.uniform(low=0.001, high=0.1, size=5)\n",
    "\n",
    "print(f\"Learning rates = {learning_rates}\")\n",
    "print(f\"Momentums = {momentums}\")\n",
    "\n",
    "for i in range(1, k + 1):\n",
    "    data = k_fold_cv(k, i, x_train, y_train)\n",
    "    optDicM[\"learning_rate\"] = learning_rates[i-1]\n",
    "    optDicM[\"momentum\"] = momentums[i-1]\n",
    "    \n",
    "    # Evaluate Momentum optimiser with varying learning rates and momentums\n",
    "    prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicM, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Momentum model on test set (unseen data)\n",
    "data = [x_train, y_train, x_test, y_test]\n",
    "optDicM[\"learning_rate\"] = learning_rates[3]\n",
    "optDicM[\"momentum\"] = momentums[3]\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicM, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openTensorBoardAtIndex(\"MLP\", \"Momentum\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation dictionary for Adagrad\n",
    "optDicAGrad = {}\n",
    "optDicAGrad[\"optMethod\"] = \"Adagrad\"\n",
    "\n",
    "initial_accum_values = np.random.uniform(low=0.01, high=0.2, size=5)\n",
    "\n",
    "print(f\"Learning rates = {learning_rates}\")\n",
    "print(f\"Initial accumulator values = {initial_accum_values}\")\n",
    "\n",
    "for i in range(1, k + 1):\n",
    "    data = k_fold_cv(k, i, x_train, y_train)\n",
    "    optDicAGrad[\"learning_rate\"] = learning_rates[i-1]\n",
    "    optDicAGrad[\"initial_accumulator_value\"] = initial_accum_values[i-1]\n",
    "    \n",
    "    # Evaluate Adagrad optimiser with varying learning rates and momentums\n",
    "    prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAGrad, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Adagrad model on test set (unseen data)\n",
    "data = [x_train, y_train, x_test, y_test]\n",
    "optDicAGrad[\"learning_rate\"] = learning_rates[3]\n",
    "optDicAGrad[\"initial_accumulator_value\"] = initial_accum_values[3]\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAGrad, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openTensorBoardAtIndex(\"MLP\", \"Adagrad\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation dictionary for RMSProp\n",
    "optDicRMS = {}\n",
    "optDicRMS[\"optMethod\"] = \"RMSProp\"\n",
    "optDicRMS[\"centered\"] = False # This normalises the weights if True\n",
    "\n",
    "decays = np.random.uniform(low=0.0, high=0.01, size=5)\n",
    "\n",
    "print(f\"Learning rates = {learning_rates}\")\n",
    "print(f\"Momentums = {momentums}\")\n",
    "print(f\"Decays = {decays}\")\n",
    "\n",
    "for i in range(1, k + 1):\n",
    "    data = k_fold_cv(k, i, x_train, y_train)\n",
    "    optDicRMS[\"learning_rate\"] = learning_rates[i-1]\n",
    "    optDicRMS[\"momentum\"] = momentums[i-1]\n",
    "    optDicRMS[\"decay\"] = decays[i-1]\n",
    "    \n",
    "    # Evaluate RMSProp optimiser with varying learning rates and momentums\n",
    "    prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicRMS, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best RMSProp model on test set (unseen data)\n",
    "data = [x_train, y_train, x_test, y_test]\n",
    "optDicRMS[\"learning_rate\"] = learning_rates[3]\n",
    "optDicRMS[\"momentum\"] = momentums[3]\n",
    "optDicRMS[\"decay\"] = decays[3]\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicRMS, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openTensorBoardAtIndex(\"MLP\", \"RMSProp\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation dictionary for Adam\n",
    "optDicAdam = {}\n",
    "optDicAdam[\"optMethod\"] = \"Adam\"\n",
    "optDicAdam[\"learning_rate\"] = 0.001\n",
    "optDicAdam[\"beta1\"] = 0.9\n",
    "optDicAdam[\"beta2\"] = 0.999\n",
    "\n",
    "beta1s = np.random.uniform(low=0.85, high=0.95, size=5)\n",
    "# Upper limit of 1.0 excluded\n",
    "beta2s = np.random.uniform(low=0.9, high=1.0, size=5)\n",
    "\n",
    "print(f\"Learning rates = {learning_rates}\")\n",
    "print(f\"Beta_1 = {beta1s}\")\n",
    "print(f\"Beta_2 = {beta2s}\")\n",
    "\n",
    "for i in range(1, k + 1):\n",
    "    data = k_fold_cv(k, i, x_train, y_train)\n",
    "    optDicAdam[\"learning_rate\"] = learning_rates[i-1]\n",
    "    optDicAdam[\"beta1\"] = beta1s[i-1]\n",
    "    optDicAdam[\"beta2\"] = beta2s[i-1]\n",
    "    \n",
    "    # Evaluate Adam optimiser with varying learning rates and momentums\n",
    "    prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAdam, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Adam model on test set (unseen data)\n",
    "data = [x_train, y_train, x_test, y_test]\n",
    "optDicAdam[\"learning_rate\"] = learning_rates[3]\n",
    "optDicAdam[\"beta1\"] = beta1s[3]\n",
    "optDicAdam[\"beta2\"] = beta2s[3]\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAdam, act=activationFunction, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openTensorBoardAtIndex(\"MLP\", \"Adam\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "\n",
    "### (b)\n",
    "Compare the performance of the network from (a) using ReLU, tanh, and sigmoid functions as the activation function.\n",
    "\n",
    "### Instructions\n",
    "* Use a **table** to present your results. \n",
    "* Discuss in **at most** 150 words the differences in the functions, and why that may have lead to different results. \n",
    "\n",
    "### Hints\n",
    "\n",
    "Don't forget to play with the parameters, as the ones above will probably throw errors!\n",
    "\n",
    "Increase 'max_steps' to increase the number of iterations. Also, use TensorBoard - it might provide some insight into what's happening during training.\n",
    "\n",
    "If you have any trouble using any of the parameters, consult the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/). The search bar up the top is really good! (I guess it's not surprising considering the API is made by Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x_train, y_train, x_test, y_test]\n",
    "\n",
    "# TESTING ALL ACTIVATION FUNCTIONS ON LAST CONFIGURATION (INDEX 4) FOR ADAM OPTIMISER FROM Q1.a)\n",
    "# Testing ReLU\n",
    "activationFunction = tf.nn.relu\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAdam, act=activationFunction, max_steps=100,\n",
    "                  path=\"ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Tanh\n",
    "activationFunction = tf.nn.tanh\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAdam, act=activationFunction, max_steps=100,\n",
    "                  path=\"Tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Sigmoid\n",
    "activationFunction = tf.nn.sigmoid\n",
    "prac7ConvMLPModel(data, model='MLP', MLPTop=MLPTopology, optimiser=optDicAdam, act=activationFunction, max_steps=100,\n",
    "                  path=\"Sigmoid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
